Annotation-model: Tests for the Web Annotation Data Model
=========================================================

The [Web Annotation Data Model](https://www.w3.org/TR/annotation-model)
specification presents a JSON-oriented collection of terms and structure
that permit the sharing of annotations about other content.

The purpose of these tests is to help validate that each of the structural
requirements expressed in the Data Model specification are properly
supported by implementations.

The general approach for this testing is to enable both manual and
automated testing. However, since the specification has no actual user
interface requirements, there is no general automation mechanism that can
be presented for clients.  Instead, the automation mechanism is one where
client implementors could take advantage of the plumbing we provide here
to push their data into the tests and collect the results of the testing.
This assumes knowledge of the requirements of each test / collection of
tests so that the input data is relevant.  Each test or test collection
contains information sufficient for the task.

Running Tests
-------------

WPT is big, but at its core is the "framework".  This includes a web
server and a rich collection of JavaScript that supports writing and
running tests and reporting results.  There is all sorts of support for
things like creating protocol emulators to test clients, Web Sockets,
encryption, IPV6, etc.

The rest of WPT is the tests for the web platform - most top level folders
map to a W3C recommendation.  Many of the tests are automated.  Some are
"ref" tests, where output is visually compared to a "reference" sample.
The rest are "manual" tests, where the tester is given a window in which
to do something.  The window can then assess whether things worked or,
in some cases, the user must make the determination and click "pass"
or "fail" in the main driver window.  For an example of this in action,
see the [w3c-test.org site](http://w3c-test.org/tools/runner/index.html)
- in the Run Tests under path, try "/2dcontext" as a value for the "run
tests under path" field. These tests are largely automated JavaScript
tests.  If you want to look at just ref or manual tests, de-select the
"JavaScript tests" checkbox.  When you are ready, click "Run".

In the case of this test collection, we will be initially creating
manual tests.  These will automatically determine pass or fail and
generate output for the main driver window.  The plan is to minimize
the number of such tests to ease the burden on the testers while still
exercising all the features.

The workflow for running these tests is something like:

1. Start up the test driver window and select the annotation-model tests - click "Start"
1. A window pops up that shows a test - the description of which tells the tester what input is expected.  The window contains a textarea into which the input can be typed / pasted, along with a button to click to start testing that input.
1. The tester (presumably in another window) brings up their annotation client and uses it to generate an annotation that supplies the requested structure.  They then copy / paste that into the aforementioned textarea and select the button.
1. The test runs.  Success or failure is determined and reported to the test driver window, which then cycles to the next test in the sequence.
1. Repeat steps 2-4 until done.
1. Download the JSON format report of test results, which can then be visually inspected, reported on using various tools, or passed on to W3C for evaluation and collection in the Implementation Report.

**Remember that while these tests are written to help exercise
implementations, their other (important) purpose is to increase
confidence that there are interoperable implementations.** So,
implementers are our audience, but these tests are not meant to be a
comprehensive collection of tests for a client that might implement
the Recommendation.  The bulk of the tests are manual because there are
no UI requirements in the Recommendation that would make it possible to
effectively stimulate every client portably.

Having said that, because the structure of these "manual" tests is very
rigid, it is possible for an implementer who understands test automation
to use an open source tool such as [Selenium](http://www.seleniumhq.org/)
to run these "manual" tests against their implementation - exercising
their implementation against content they provide to create annotations
and feed the data into our test input field and run the test.


Capturing and Reporting Results
-------------------------------

As tests are run against implementations, if the results of testing
are submitted to [test-results](https://w3c.github.io/test-results/)
then they will be automatically included in documents generated by
[wptreport](https://www.github.com/w3c/wptreport). The same tool can be
used locally to view reports about recorded results.


Test Cases
----------

Each test is expressed as a simple requirement in a test file.  For each
section of the document, the simple requirement is represented as a
structure that describes the nature of the test, and then includes or
references minimal JSON Schema that test the assertions.

The structure of a test case is defined using a [JSON-LD Context](JSONtest-v1.jsonld).
That context defines the following terms:

|Keyword        | Values          | Meaning
|---------------|-----------------|---------
|name           | string          | The name of this test for display purposes
|description    | string          | A long self-describing paragraph that explains the purpose of the test and the expected input
|ref            | URI             | An optional reference to the portion of the specification to which the test relates
|testType       | `automated`, `manual`, `ref` | The type of test - this informs [WPT](https://github.com/w3c/web-platform-tests) how the test should be controlled and presented
|assertions     | list of URI, List, or AssertionObject | The ordered collection of tests the input should be run against. See [JSON Schema Usage](#jsonSchema) for the structure of the objects.  URI is relative to the top level folder of the test collection if it has a slash; relative to the current directory if it does not. Lists can be nested to define groups of sub-tests.  Assertions / groups can be conditionally skipped.  See [Assertion Lists](#assertionLists) for more details.

Each test case has a suffix of ".test" and a shape like:

<pre>
{
  "@context": "https://www.w3.org/ns/JSONtest-v1.jsonld",
  "name": "Verify annotation conforms to the model",
  "description": "Supply an example annotation that conforms to the basic structure.",
  "ref": "https://www.w3.org/TR/annotation-model/#model",
  "testType": "manual",
  "assertions": [
    "common/has_context.json",
    "common/has_id.json",
    {
      "$schema": "http://json-schema.org/draft-04/schema#",
      "title": "Verify annotation has target",
      "type": "object",
      "expectedResult": "valid",
      "errorMessage": "The object was missing a required 'target' property",
      "properties": {
        "target": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "array",
              "anyOf": [
                {
                  "type": "string"
                }
              ]
            }
          ],
          "not": {"type": "object"}
        }
      },
      "required": ["target"]
    }
  ]
}
</pre>

External references are used when the "assertion" is a common one that needs to be
checked on many different test cases (e.g., that there is an @context in the supplied
annotation).

### <a id="assertionLists">Assertion Lists</a> ###

The `assertion` list is an ordered list of assertions that will be evaluated against the
submitted content. The list is *required*, and MUST have at least one entry. Entries in the
list have the following types:

* URI

  A relative or absolute URI that references a AssertionObject in a .json file.  If the URI
  is relative but contains no slashes, then it is considered to be in the current directory.
  If the URI is relative, contains slashes, but **does not start with a slash** then it is
  considered relative to the top of the tree of the current test collection (e.g.,
  `annotation-model`).
* AssertionObject

  An in-line Object as defined in the section


<a id="assertionObject">Assertion Objects</a>
-----------------

In this collection of tests, Assertion Objects can be contained inline in the `.test` files
or contained in external files with the suffix `.json`.
The vocabularly and structure is as defined in
[JSON Schema v4](http://json-schema.org/documentation.html) augmented with some additional
properties defined in this section.

In general each JSON Schema definition provided in this test suite should be
as minimal as possible.  This promotes clarity and increases the likelihood that it is
correct.  While it is ---possible--- to create JSON Schema files that enforce many different
requirements on a data model, it increases the complexity and can also reduce the atomicity
of tests / sub-tests (because a test ends up testing more than one thing).  Please try
to avoid creating complex JSON Schema.  (A notable exception is the situation where
multiple properties of a structure are interdependent.)

In addition to the validation keys defined in JSON Schema v4, Schema files in this collection
are also permitted to use the following keywords:

|Keyword        | Values          | Meaning |
|---------------|-----------------|---------|
|actionOnUnexpectedResult   | @@@TODO@@@      | Action to take when the result is not as expected. Default is `failAndContinue` |
|assertionType  | `must`, `may`, `should` | Informs the system about the severity of a failure. The default is `must` |
|errorMessage   | string          | A human readable explanation of what it means if the test fails.  |
|expectedResult | `valid`, `invalid`  | Tells the framework whether validating against this schema is expected to succeed or fail.  The default is `valid` |


### Example Assertion Object ###

<pre>
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "title": "Verify annotation has @context",
  "type": "object",
  "expectedResult" : "valid",
  "properties": {
    "@context": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "array",
          "anyOf": [
            {
              "type": "string"
            }
          ]
        }
      ],
      "not": {"type": "object"}
    }
  },
  "required": ["@context"]
}
</pre>

Note that in the case where a feature is *optional* the JSON Schema MUST be crafted such
that if the attribute is permitted to be missing from the content (so that the result is `true`),
but when the attribute is present in the content it conforms to any requirements.

@@@ example of optional attribute @@@


Automating Test Execution
-------------------------


Command Line Tools
------------------

### Building the Test Files ###

### Testing the Tests ###

### Driving Tests with Input Files ###
